{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG6-QVbVHVFX",
        "outputId": "24ba6095-d62f-4746-e80c-8c9230e860a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading data...\n",
            "Train: (300, 19928), Test: (304, 19928)\n",
            "Checking for nulls in training data:\n",
            "Null counts:\n",
            "PreInt_Demos_Fam_Child_Ethnicity     5\n",
            "MRI_Track_Age_at_Scan               97\n",
            "dtype: int64\n",
            "\n",
            "Checking for nulls in test data:\n",
            "Null counts:\n",
            "PreInt_Demos_Fam_Child_Ethnicity     3\n",
            "PreInt_Demos_Fam_Child_Race          6\n",
            "Barratt_Barratt_P1_Edu               1\n",
            "Barratt_Barratt_P1_Occ               1\n",
            "Barratt_Barratt_P2_Edu              36\n",
            "Barratt_Barratt_P2_Occ              42\n",
            "EHQ_EHQ_Total                        1\n",
            "ColorVision_CV_Score                 9\n",
            "APQ_P_APQ_P_CP                      15\n",
            "APQ_P_APQ_P_ID                      15\n",
            "APQ_P_APQ_P_INV                     15\n",
            "APQ_P_APQ_P_OPD                     15\n",
            "APQ_P_APQ_P_PM                      15\n",
            "APQ_P_APQ_P_PP                      15\n",
            "SDQ_SDQ_Conduct_Problems            30\n",
            "SDQ_SDQ_Difficulties_Total          30\n",
            "SDQ_SDQ_Emotional_Problems          30\n",
            "SDQ_SDQ_Externalizing               30\n",
            "SDQ_SDQ_Generating_Impact           30\n",
            "SDQ_SDQ_Hyperactivity               30\n",
            "SDQ_SDQ_Internalizing               30\n",
            "SDQ_SDQ_Peer_Problems               30\n",
            "SDQ_SDQ_Prosocial                   30\n",
            "dtype: int64\n",
            "\n",
            "Performing exploratory data analysis...\n",
            "Number of features to be log-transformed: 18\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "import matplotlib.pyplot as plt #visualization settings\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"Set2\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# set paths-- change to YOUR path\n",
        "DATA_FOLDER = '/content/drive/MyDrive/Colab Notebooks/data/'\n",
        "FIGURE_FOLDER = '/content/drive/MyDrive/Colab Notebooks/img/'\n",
        "RESULT_FOLDER = '/content/drive/MyDrive/Colab Notebooks/results/'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "import os\n",
        "os.makedirs(FIGURE_FOLDER, exist_ok=True)\n",
        "os.makedirs(RESULT_FOLDER, exist_ok=True)\n",
        "\n",
        "def check_for_nulls(df):\n",
        "    \"\"\"Print out null counts for a dataframe.\"\"\"\n",
        "    null_counts = df.isnull().sum()\n",
        "    if null_counts.sum() > 0:\n",
        "        print(\"Null counts:\")\n",
        "        print(null_counts[null_counts > 0])\n",
        "    else:\n",
        "        print(\"No nulls found.\")\n",
        "\n",
        "def load_data(mode='TRAIN', sample_size=None):\n",
        "    \"\"\"\n",
        "    Load and merge all relevant data files.\n",
        "\n",
        "    Parameters:\n",
        "    mode (str): 'TRAIN' or 'TEST'\n",
        "    sample_size (int): If provided, sample the data to speed up processing\n",
        "\n",
        "    Returns:\n",
        "    DataFrame with all features combined\n",
        "    \"\"\"\n",
        "    # Load categorical metadata - handle different naming conventions between TRAIN and TEST\n",
        "    if mode == 'TRAIN':\n",
        "        cat_meta = pd.read_excel(f\"{DATA_FOLDER}{mode}/{mode}_CATEGORICAL_METADATA.xlsx\")\n",
        "    else:  # TEST mode\n",
        "        cat_meta = pd.read_excel(f\"{DATA_FOLDER}{mode}/{mode}_CATEGORICAL.xlsx\")\n",
        "\n",
        "    # Load quantitative metadata\n",
        "    quant_meta = pd.read_excel(f\"{DATA_FOLDER}{mode}/{mode}_QUANTITATIVE_METADATA.xlsx\")\n",
        "\n",
        "    # Load functional connectome matrices (fix spelling)\n",
        "    fcm = pd.read_csv(f\"{DATA_FOLDER}{mode}/{mode}_FUNCTIONAL_CONNECTOME_MATRICES.csv\")\n",
        "\n",
        "    # Optional sampling for faster development/testing\n",
        "    if sample_size and mode == 'TRAIN':\n",
        "        cat_meta = cat_meta.sample(sample_size, random_state=42)\n",
        "        participant_ids = cat_meta['participant_id'].values\n",
        "        quant_meta = quant_meta[quant_meta['participant_id'].isin(participant_ids)]\n",
        "        fcm = fcm[fcm['participant_id'].isin(participant_ids)]\n",
        "\n",
        "    # Merge all data sources\n",
        "    data = cat_meta.merge(quant_meta, on='participant_id', how='left')\n",
        "    data = data.merge(fcm, on='participant_id', how='left')\n",
        "\n",
        "    return data\n",
        "\n",
        "# Section 1--Data Loading and Exploration =====\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# Use a smaller sample size for faster development\n",
        "SAMPLE_SIZE = 300  # Set to None to use full dataset\n",
        "\n",
        "# Load training data\n",
        "train = load_data(mode='TRAIN', sample_size=SAMPLE_SIZE)\n",
        "y = pd.read_excel(f\"{DATA_FOLDER}TRAIN/TRAINING_SOLUTIONS.xlsx\")\n",
        "\n",
        "# If we sampled the train data, filter y accordingly\n",
        "if SAMPLE_SIZE:\n",
        "    y = y[y['participant_id'].isin(train['participant_id'])]\n",
        "\n",
        "# Load test data - we can use a small sample for testing, then run on full dataset for final submission\n",
        "test = load_data(mode='TEST', sample_size=100)  # Sample for development\n",
        "print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
        "\n",
        "# Load sample submission\n",
        "sub = pd.read_excel(f\"{DATA_FOLDER}SAMPLE_SUBMISSION.xlsx\")\n",
        "\n",
        "# Set participant_id as index for easier handling\n",
        "train.set_index('participant_id', inplace=True)\n",
        "test.set_index('participant_id', inplace=True)\n",
        "y_train = y.set_index('participant_id')\n",
        "\n",
        "# Define targets and features\n",
        "targets = ['ADHD_Outcome', 'Sex_F']\n",
        "features = test.columns\n",
        "\n",
        "# Check for nulls in the data\n",
        "print(\"Checking for nulls in training data:\")\n",
        "check_for_nulls(train)\n",
        "print(\"\\nChecking for nulls in test data:\")\n",
        "check_for_nulls(test)\n",
        "\n",
        "#Section 2-- Exploratory Data Analysis\n",
        "print(\"\\nPerforming exploratory data analysis...\")\n",
        "\n",
        "# Plot target distributions\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "for col, ax in zip(y_train.columns, axs):\n",
        "    counts = y_train[col].value_counts()\n",
        "    ax.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    ax.set_title(f'{col} Distribution')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURE_FOLDER}target_distributions.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Identify features for log transformation\n",
        "log_features = [f for f in features if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0.5]\n",
        "print(f\"Number of features to be log-transformed: {len(log_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 2--Exploratory Data Analysis\n",
        "print(\"\\nEDA continued...\")\n",
        "\n",
        "# Plot target distributions\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "for col, ax in zip(y_train.columns, axs):\n",
        "    counts = y_train[col].value_counts()\n",
        "    ax.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    ax.set_title(f'{col} Distribution')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURE_FOLDER}target_distributions.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Identify features for log transformation\n",
        "log_features = [f for f in features if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0.5]\n",
        "print(f\"Number of features to be log-transformed: {len(log_features)}\")\n",
        "\n",
        "# Look at distribution of numerical features with highest skew (optional visualization)\n",
        "high_skew_features = sorted([(f, scipy.stats.skew(train[f])) for f in features\n",
        "                            if (train[f] >= 0).all() and not pd.isna(scipy.stats.skew(train[f]))],\n",
        "                            key=lambda x: abs(x[1]), reverse=True)[:5]\n",
        "print(\"\\nTop 5 most skewed features:\")\n",
        "for feat, skew_val in high_skew_features:\n",
        "    print(f\"{feat}: {skew_val:.4f}\")\n",
        "\n",
        "# Section 3 -- Feature Engineering\n",
        "print(\"\\nPerforming feature engineering...\")\n",
        "\n",
        "# Handle missing values strategically before further processing\n",
        "print(\"Handling missing values...\")\n",
        "\n",
        "# Identify categorical and numerical columns based on data types\n",
        "categorical_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# For numerical columns with nulls, impute with median (more robust to outliers than mean)\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "train_num = pd.DataFrame(\n",
        "    num_imputer.fit_transform(train[numerical_cols]),\n",
        "    columns=numerical_cols,\n",
        "    index=train.index\n",
        ")\n",
        "\n",
        "# For categorical columns with nulls, impute with most frequent value\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "if categorical_cols:\n",
        "    train_cat = pd.DataFrame(\n",
        "        cat_imputer.fit_transform(train[categorical_cols]),\n",
        "        columns=categorical_cols,\n",
        "        index=train.index\n",
        "    )\n",
        "    # Combine imputed data back\n",
        "    train_imputed = pd.concat([train_num, train_cat], axis=1)\n",
        "else:\n",
        "    train_imputed = train_num\n",
        "\n",
        "# Apply the same imputation to test data\n",
        "test_num = pd.DataFrame(\n",
        "    num_imputer.transform(test[numerical_cols]),\n",
        "    columns=numerical_cols,\n",
        "    index=test.index\n",
        ")\n",
        "if categorical_cols:\n",
        "    test_cat = pd.DataFrame(\n",
        "        cat_imputer.transform(test[categorical_cols]),\n",
        "        columns=categorical_cols,\n",
        "        index=test.index\n",
        "    )\n",
        "    test_imputed = pd.concat([test_num, test_cat], axis=1)\n",
        "else:\n",
        "    test_imputed = test_num\n",
        "\n",
        "# Function to create interaction features for given columns\n",
        "def create_interaction_features(df, cols):\n",
        "    \"\"\"Create interaction terms between specified columns.\"\"\"\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create meaningful pairwise interactions\n",
        "    for i, col1 in enumerate(cols):\n",
        "        for col2 in cols[i+1:]:\n",
        "            df_copy[f\"{col1}_{col2}_mult\"] = df[col1] * df[col2]\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "# Create features for SDQ and APQ columns specifically - these are known to be important for ADHD prediction\n",
        "# from clinical research (Strengths and Difficulties Questionnaire & Alabama Parenting Questionnaire)\n",
        "sdq_cols = [col for col in train_imputed.columns if 'SDQ_' in col]\n",
        "apq_cols = [col for col in train_imputed.columns if 'APQ_' in col]\n",
        "\n",
        "print(f\"Number of SDQ features: {len(sdq_cols)}\")\n",
        "print(f\"Number of APQ features: {len(apq_cols)}\")\n",
        "\n",
        "# Identify most important features for each target using SelectKBest\n",
        "def get_important_features(X, y, k=20):\n",
        "    \"\"\"Get the k most important features for target y.\"\"\"\n",
        "    selector = SelectKBest(f_classif, k=k)\n",
        "    selector.fit(X, y)\n",
        "    mask = selector.get_support()\n",
        "    return list(X.columns[mask])\n",
        "\n",
        "# Get important features for each target\n",
        "adhd_features = get_important_features(train_imputed, y_train['ADHD_Outcome'], k=40)\n",
        "sex_features = get_important_features(train_imputed, y_train['Sex_F'], k=40)\n",
        "\n",
        "# Print some insights about the important features\n",
        "print(\"\\nTop 5 important features for ADHD prediction:\")\n",
        "for feature in adhd_features[:5]:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "print(\"\\nTop 5 important features for Sex prediction:\")\n",
        "for feature in sex_features[:5]:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "# Combine unique important features from both targets\n",
        "important_features = list(set(adhd_features + sex_features + sdq_cols + apq_cols))\n",
        "print(f\"\\nNumber of selected important features: {len(important_features)}\")\n",
        "\n",
        "# Create interaction features for both train and test - limit to most important features to avoid explosion\n",
        "top_features_for_interactions = adhd_features[:3] + sex_features[:3]\n",
        "top_features_for_interactions = list(set(top_features_for_interactions))\n",
        "\n",
        "X_train_int = create_interaction_features(train_imputed[important_features], top_features_for_interactions)\n",
        "X_test_int = create_interaction_features(test_imputed[important_features], top_features_for_interactions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bO1kW3zLmWu",
        "outputId": "6c584082-6207-4cb2-d319-226d0d8c5f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EDA continued...\n",
            "Number of features to be log-transformed: 18\n",
            "\n",
            "Top 5 most skewed features:\n",
            "ColorVision_CV_Score: -4.3239\n",
            "APQ_P_APQ_P_INV: -2.6976\n",
            "APQ_P_APQ_P_PP: -2.6966\n",
            "Barratt_Barratt_P1_Edu: -1.9190\n",
            "APQ_P_APQ_P_CP: 1.4402\n",
            "\n",
            "Performing feature engineering...\n",
            "Handling missing values...\n",
            "Number of SDQ features: 9\n",
            "Number of APQ features: 6\n",
            "\n",
            "Top 5 important features for ADHD prediction:\n",
            "- 2throw_98thcolumn\n",
            "- 4throw_145thcolumn\n",
            "- 5throw_113thcolumn\n",
            "- 5throw_157thcolumn\n",
            "- 7throw_77thcolumn\n",
            "\n",
            "Top 5 important features for Sex prediction:\n",
            "- 0throw_79thcolumn\n",
            "- 2throw_198thcolumn\n",
            "- 4throw_14thcolumn\n",
            "- 4throw_112thcolumn\n",
            "- 6throw_190thcolumn\n",
            "\n",
            "Number of selected important features: 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 4-- Model Building and Evaluation\n",
        "print(\"\\nBuilding and evaluating models...\")\n",
        "\n",
        "# Split data for validation\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_int, y_train, test_size=0.25, random_state=42, stratify=y_train['ADHD_Outcome']\n",
        ")\n",
        "\n",
        "# Define different model configurations to test\n",
        "models = {\n",
        "    'RidgeClassifier': MultiOutputClassifier(RidgeClassifier(alpha=10, class_weight='balanced', random_state=42)),\n",
        "    'LogisticRegression': MultiOutputClassifier(LogisticRegression(max_iter=1000, class_weight='balanced',\n",
        "                                                                C=0.1, solver='liblinear', random_state=42)),\n",
        "    'RandomForest': MultiOutputClassifier(RandomForestClassifier(n_estimators=100, max_depth=15,\n",
        "                                                              class_weight='balanced', random_state=42)),\n",
        "    'GradientBoosting': MultiOutputClassifier(GradientBoostingClassifier(n_estimators=100, learning_rate=0.05,\n",
        "                                                                      max_depth=5, random_state=42))\n",
        "}\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "def create_pipeline(model, n_components=50):\n",
        "    \"\"\"Create a preprocessing and model pipeline.\"\"\"\n",
        "    # Since we've already handled missing values above, we can simplify this pipeline\n",
        "    return make_pipeline(\n",
        "        ColumnTransformer([\n",
        "            ('log_transform', FunctionTransformer(np.log1p),\n",
        "             [col for col in X_train_int.columns if col in log_features])\n",
        "        ], remainder='passthrough'),\n",
        "        StandardScaler(),\n",
        "        PCA(n_components=n_components, random_state=42),\n",
        "        model\n",
        "    )\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    # Create and fit pipeline\n",
        "    pipeline = create_pipeline(model, n_components=min(50, X_train_split.shape[0] - 1))\n",
        "    pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = pipeline.predict(X_val_split)\n",
        "\n",
        "    # Calculate metrics\n",
        "    f1_adhd = f1_score(y_val_split['ADHD_Outcome'], y_pred[:, 0])\n",
        "    f1_sex = f1_score(y_val_split['Sex_F'], y_pred[:, 1])\n",
        "    f1_avg = (f1_adhd + f1_sex) / 2\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'f1_adhd': f1_adhd,\n",
        "        'f1_sex': f1_sex,\n",
        "        'f1_avg': f1_avg,\n",
        "        'pipeline': pipeline\n",
        "    }\n",
        "\n",
        "    print(f\"{name} F1 Scores - ADHD: {f1_adhd:.4f}, Sex: {f1_sex:.4f}, Average: {f1_avg:.4f}\")\n",
        "    print(classification_report(y_val_split, y_pred, target_names=targets))\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results.keys(), key=lambda k: results[k]['f1_avg'])\n",
        "best_pipeline = results[best_model_name]['pipeline']\n",
        "print(f\"\\nBest model: {best_model_name} with average F1 score: {results[best_model_name]['f1_avg']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YneqE8_LgYt",
        "outputId": "6e50beae-1238-452a-fae6-d1dcb029dc6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Building and evaluating models...\n",
            "\n",
            "Training RidgeClassifier...\n",
            "RidgeClassifier F1 Scores - ADHD: 0.8991, Sex: 0.9206, Average: 0.9099\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ADHD_Outcome       0.88      0.92      0.90        53\n",
            "       Sex_F       0.94      0.91      0.92        32\n",
            "\n",
            "   micro avg       0.90      0.92      0.91        85\n",
            "   macro avg       0.91      0.92      0.91        85\n",
            "weighted avg       0.90      0.92      0.91        85\n",
            " samples avg       0.81      0.81      0.80        85\n",
            "\n",
            "\n",
            "Training LogisticRegression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression F1 Scores - ADHD: 0.8972, Sex: 0.9206, Average: 0.9089\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ADHD_Outcome       0.89      0.91      0.90        53\n",
            "       Sex_F       0.94      0.91      0.92        32\n",
            "\n",
            "   micro avg       0.91      0.91      0.91        85\n",
            "   macro avg       0.91      0.91      0.91        85\n",
            "weighted avg       0.91      0.91      0.91        85\n",
            " samples avg       0.81      0.79      0.79        85\n",
            "\n",
            "\n",
            "Training RandomForest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest F1 Scores - ADHD: 0.8870, Sex: 0.4651, Average: 0.6760\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ADHD_Outcome       0.82      0.96      0.89        53\n",
            "       Sex_F       0.91      0.31      0.47        32\n",
            "\n",
            "   micro avg       0.84      0.72      0.77        85\n",
            "   macro avg       0.87      0.64      0.68        85\n",
            "weighted avg       0.86      0.72      0.73        85\n",
            " samples avg       0.75      0.65      0.68        85\n",
            "\n",
            "\n",
            "Training GradientBoosting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoosting F1 Scores - ADHD: 0.9074, Sex: 0.7308, Average: 0.8191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ADHD_Outcome       0.89      0.92      0.91        53\n",
            "       Sex_F       0.95      0.59      0.73        32\n",
            "\n",
            "   micro avg       0.91      0.80      0.85        85\n",
            "   macro avg       0.92      0.76      0.82        85\n",
            "weighted avg       0.91      0.80      0.84        85\n",
            " samples avg       0.77      0.71      0.73        85\n",
            "\n",
            "\n",
            "Best model: RidgeClassifier with average F1 score: 0.9099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 5--Final Model and Submission\n",
        "print(\"\\nTraining final model and generating submission...\")\n",
        "\n",
        "# training best model on full training data\n",
        "final_pipeline = create_pipeline(\n",
        "    models[best_model_name],\n",
        "    n_components=min(100, train.shape[0] - 1)\n",
        ")\n",
        "final_pipeline.fit(X_train_int, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = final_pipeline.predict(X_test_int)\n",
        "\n",
        "# Save predictions\n",
        "sub_df = pd.DataFrame({\n",
        "    'participant_id': test.index,\n",
        "    'ADHD_Outcome': y_pred[:, 0],\n",
        "    'Sex_F': y_pred[:, 1]\n",
        "})\n",
        "sub_df.to_csv(f'{RESULT_FOLDER}submission.csv', index=False)\n",
        "print(f\"Submission saved to {RESULT_FOLDER}submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0HRpRwJLYBB",
        "outputId": "d9b965ba-b771-4501-f575-a0098b240382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training final model and generating submission...\n",
            "Submission saved to /content/drive/MyDrive/Colab Notebooks/results/submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 6-- Feature Importance Analysis\n",
        "print(\"\\nAnalyzing feature importance...\")\n",
        "\n",
        "if best_model_name in ['RandomForest', 'GradientBoosting']:\n",
        "    # Get feature importance for tree-based models\n",
        "    importances = np.zeros(X_train_int.shape[1])\n",
        "\n",
        "    for i, estimator in enumerate(final_pipeline[-1].estimators_):\n",
        "        if hasattr(estimator, 'feature_importances_'):\n",
        "            importances += estimator.feature_importances_\n",
        "\n",
        "    importances = importances / len(final_pipeline[-1].estimators_)\n",
        "\n",
        "    # Plot top 20 features\n",
        "    indices = np.argsort(importances)[::-1][:20]\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.title(f'Top 20 Feature Importances for {best_model_name}')\n",
        "    plt.barh(range(20), importances[indices])\n",
        "    plt.yticks(range(20), X_train_int.columns[indices])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{FIGURE_FOLDER}feature_importance.png\", dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "print(\"Analysis complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jCrCaaHLahe",
        "outputId": "e95a03d4-eb0d-468b-f0ac-d17f74f91973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing feature importance...\n",
            "Analysis complete!\n"
          ]
        }
      ]
    }
  ]
}